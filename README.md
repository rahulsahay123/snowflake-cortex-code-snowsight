# â„ï¸ Snowflake Cortex Code â€” Snowsight Demo

> *From zero documentation to a fully working dbt pipeline â€” using only natural language prompts in Snowflake Cortex Code.*

---

## ğŸ“½ï¸ Demo Video

ğŸ¬ **[Watch the Demo on LinkedIn](#)**
*(Video coming soon â€” placeholder)*

---

## ğŸ¯ What is this?

This repo contains a hands-on demo of **Snowflake Cortex Code in Snowsight** â€” showing how a data engineer can:

- Discover and understand an unknown schema from scratch
- Profile data without writing a single line of SQL manually
- Run complex multi-table business analysis queries
- Build a production-ready dbt pipeline

All of this using **plain english natural language prompts** â€” no documentation, no schema knowledge, no manual SQL writing.

---

## ğŸ‘¤ Persona

**Day 1 Data Engineer.** New to the team. No documentation. No ERD. No senior engineer available. Just 8 tables and a deadline.

---

## ğŸ“¦ Dataset

| Property | Value |
|---|---|
| Database | SNOWFLAKE_SAMPLE_DATA |
| Schema | TPCH_SF1 |
| Domain | Supply Chain |
| Tables | 8 |
| Total Rows | ~8.6 Million |

### Tables Overview

| Table | Type | Row Count |
|---|---|---|
| LINEITEM | Fact | 6,001,215 |
| ORDERS | Fact | 1,500,000 |
| PARTSUPP | Bridge | 800,000 |
| PART | Dimension | 200,000 |
| CUSTOMER | Dimension | 150,000 |
| SUPPLIER | Dimension | 10,000 |
| NATION | Dimension | 25 |
| REGION | Dimension | 5 |

---

## ğŸ—‚ï¸ Repo Structure

```
snowflake-cortex-code-snowsight/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ demo_video_placeholder.md      â† demo video link
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ 00_context_setting.md
â”‚   â”œâ”€â”€ 01_table_discovery.md
â”‚   â”œâ”€â”€ 02_object_inventory.md
â”‚   â”œâ”€â”€ 03_documentation_check.md
â”‚   â”œâ”€â”€ 04_data_dictionary.md
â”‚   â”œâ”€â”€ 05_data_profiling.md
â”‚   â”œâ”€â”€ 06_business_analysis.md
â”‚   â””â”€â”€ 07_dbt_pipeline.md
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 00_context_setting.sql
â”‚   â”œâ”€â”€ 01_table_discovery.sql
â”‚   â”œâ”€â”€ 02_object_inventory.sql
â”‚   â”œâ”€â”€ 03_documentation_check.sql
â”‚   â”œâ”€â”€ 04_data_dictionary.sql
â”‚   â”œâ”€â”€ 05_data_profiling.sql
â”‚   â””â”€â”€ 06_business_analysis.sql
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ testdbt/
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â”œâ”€â”€ profiles.yml
â”‚       â”œâ”€â”€ packages.yml
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ staging/
â”‚           â”‚   â”œâ”€â”€ sources.yml
â”‚           â”‚   â””â”€â”€ stg_orders.sql
â”‚           â”œâ”€â”€ intermediate/
â”‚           â”‚   â””â”€â”€ int_orders_enriched.sql
â”‚           â””â”€â”€ mart/
â”‚               â””â”€â”€ mart_revenue_by_segment.sql
â””â”€â”€ wiki/
    â””â”€â”€ Home.md
```

---

## ğŸš€ Demo Flow

### Block 0 â€” Context Setting
Set working context â€” role, warehouse, database and schema.

### Block 1 â€” Table Discovery
Discover all tables and row counts using INFORMATION_SCHEMA â€” zero table scans.

### Block 2 â€” Object Inventory
Check for existing views, functions and stored procedures.

### Block 3 â€” Documentation Check
Verify if any table or column comments exist.

### Block 4 â€” Data Dictionary
Generate a complete data dictionary with business descriptions using column naming conventions.

### Block 5 â€” Data Profiling
Profile the ORDERS table â€” null counts, distinct values, min/max, date ranges.

### Block 6 â€” Business Analysis
Three multi-table business analysis queries â€” all generated from plain english:

| Query | Tables Joined | Insight |
|---|---|---|
| Revenue by Market Segment & Region | 5 tables | Net revenue by segment and geography |
| Delayed Shipments by Supplier | 3 tables | Top 10 suppliers causing delays |
| Part Supply Margin Analysis | 2 tables | Margin % by part type and brand |

### Block 7 â€” dbt Pipeline
Three layer dbt pipeline â€” all generated by Cortex Code:

| Model | Layer | Type |
|---|---|---|
| stg_orders | Staging | View |
| int_orders_enriched | Intermediate | View |
| mart_revenue_by_segment | Mart | Table |

---

## ğŸ’¡ Key Insights

**1. Naming Convention = AI Readiness**
Cortex Code uses column prefixes (`C_`, `O_`, `L_`) to understand context and generate accurate business descriptions. Good naming conventions are the first layer of AI readiness in your data platform.

**2. INFORMATION_SCHEMA First**
Always use INFORMATION_SCHEMA for metadata queries â€” zero table scans, near zero cost, instant results.

**3. Business Language Over Technical Language**
You don't need to know column names. Describe what you want in business terms â€” Cortex figures out the right tables and columns automatically.

**4. Human in the Loop**
Cortex Code shows a diff view before applying changes. You review, you approve. AI accelerates, humans decide.

---

## ğŸ“Š ROI Summary

| Task | Traditional | With Cortex Code |
|---|---|---|
| Schema understanding | 2-3 hours | 30 seconds |
| Data dictionary | Days | 2 minutes |
| Business queries | Hours | Seconds |
| dbt pipeline setup | Half a day | 15 minutes |
| **Total** | **~5 hours** | **~20 minutes** |

---

## ğŸ› ï¸ Prerequisites

- Snowflake account with Cortex Code enabled
- Cross-region inference enabled
- ACCOUNTADMIN role or equivalent
- Access to SNOWFLAKE_SAMPLE_DATA database
- dbt project in Snowsight (for Block 7)

---

## ğŸ“ How to Use This Repo

1. Open Snowflake Snowsight
2. Enable Cortex Code from the left panel
3. Set context using prompts in `/prompts/00_context_setting.md`
4. Follow prompts sequentially â€” each folder has the prompt and the expected SQL
5. Validate output against SQL files in `/sql/` folder
6. For dbt â€” copy files from `/dbt/testdbt/` into your Snowsight dbt project

---

## ğŸ“– Medium Article

ğŸ“„ **[Read the full walkthrough on Medium](#)**
*(Coming soon)*

---

## ğŸ¤ Connect

**Author:** Rahul Sahay
**LinkedIn:** [linkedin.com/in/rahulsahay](#)
**GitHub:** [github.com/rahulsahay123](https://github.com/rahulsahay123)

---

## â­ If this demo was useful, give the repo a star!
