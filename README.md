# â„ï¸ Snowflake Cortex Code â€” Snowsight Demo

> *What if you could explore, profile and build a full data pipeline just by describing what you want â€” in plain english?*
> *That's exactly what this demo shows.*

---

## ğŸ“½ï¸ Demo Video

ğŸ¬ **[Watch the Demo on LinkedIn](#)**
*(Video coming soon)*

---

## ğŸ¯ What is this?

This repo is a **prompt-first guide** to Snowflake Cortex Code in Snowsight.

Every SQL script in this repo was generated by typing a plain english prompt into the Cortex Code panel â€” no manual SQL writing, no schema memorization, no documentation needed.

The focus is on **how to write the right prompt** â€” not on the SQL output itself.

---

## ğŸ‘¤ Persona

**Day 1 Data Engineer.** New team. 8 tables. No documentation. No ERD. Boss wants a pipeline by EOD.

This demo shows how Cortex Code becomes your co-pilot from the first prompt to a running dbt pipeline.

---

## ğŸ“¦ Dataset

| Property | Value |
|---|---|
| Database | SNOWFLAKE_SAMPLE_DATA |
| Schema | TPCH_SF1 |
| Domain | Supply Chain |
| Tables | 8 |
| Total Rows | ~8.6 Million |

---

## ğŸ—‚ï¸ Repo Structure

```
snowflake-cortex-code-snowsight/
â”œâ”€â”€ README.md
â”œâ”€â”€ prompts/                           â† START HERE â€” all prompts
â”‚   â”œâ”€â”€ 00_context_setting.md
â”‚   â”œâ”€â”€ 01_table_discovery.md
â”‚   â”œâ”€â”€ 02_object_inventory.md
â”‚   â”œâ”€â”€ 03_documentation_check.md
â”‚   â”œâ”€â”€ 04_data_dictionary.md
â”‚   â”œâ”€â”€ 05_data_profiling.md
â”‚   â”œâ”€â”€ 06_business_analysis.md
â”‚   â””â”€â”€ 07_dbt_pipeline.md
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ cortex_code_demo_complete.sql  â† generated output for reference only
â””â”€â”€ dbt/testdbt/
    â”œâ”€â”€ dbt_project.yml
    â”œâ”€â”€ profiles.yml
    â””â”€â”€ models/
        â”œâ”€â”€ staging/
        â”‚   â”œâ”€â”€ sources.yml
        â”‚   â””â”€â”€ stg_orders.sql
        â”œâ”€â”€ intermediate/
        â”‚   â””â”€â”€ int_orders_enriched.sql
        â””â”€â”€ mart/
            â””â”€â”€ mart_revenue_by_segment.sql
```

> ğŸ’¡ **The `/prompts` folder is the heart of this repo.** Each file contains the exact prompt, what to expect and key talking points. The `/sql` folder is the generated output â€” for reference only.

---

## ğŸš€ Demo Flow â€” Prompt by Prompt

### ğŸŸ¦ Block 0 â€” Set Context
**One prompt. Four commands. Zero ambiguity.**

> *"Before I begin, set my working context: role = ACCOUNTADMIN, warehouse = COMPUTE_WH, database = SNOWFLAKE_SAMPLE_DATA, schema = TPCH_SF1. Confirm each step."*

---

### ğŸŸ¦ Block 1 â€” Discover Tables
**Ask Cortex what exists â€” without opening a single table.**

> *"How many tables are in the TPCH_SF1 schema? Show me the record count for each table in a single query, ordered by row count descending."*

âœ… Cortex chose INFORMATION_SCHEMA over full table scans â€” zero compute cost.

---

### ğŸŸ¦ Block 2 â€” Object Inventory
**Check if anyone left anything behind.**

> *"In the current schema TPCH_SF1, show me the count of user defined views, functions and stored procedures grouped by object type in a single query."*

âœ… All zeros. Clean slate confirmed.

---

### ğŸŸ¦ Block 3 â€” Documentation Check
**Confirm there is nothing to read.**

> *"Check if any tables or columns in TPCH_SF1 have comments or descriptions defined. Show table name, column name and the comment in a single query."*

âœ… Zero rows. No documentation exists. Now watch what happens next.

---

### ğŸŸ¦ Block 4 â€” Data Dictionary
**From zero docs to a full data dictionary â€” in one prompt.**

> *"Generate a complete data dictionary for all tables in TPCH_SF1 schema showing table name, column name, data type, nullable flag, ordinal position and a business description for each column. Use INFORMATION_SCHEMA for structure and your knowledge of the TPCH data model to generate meaningful business descriptions. Cover all 8 tables. Order by table name and ordinal position."*

âœ… 61 columns across 8 tables with business descriptions â€” generated from column naming conventions alone.

> ğŸ’¡ **Key Insight:** Cortex used column prefixes like C_, O_, L_ to understand business context. Your naming convention is the first layer of AI readiness in your data platform.

---

### ğŸŸ¦ Block 5 â€” Data Profiling
**Profile 1.5 million rows â€” with one prompt.**

> *"Profile the ORDERS table in TPCH_SF1 schema using a single query. For each column show: column name, data type, total row count, null count, null percentage, distinct value count. For numeric columns additionally show min, max and average. For date columns show earliest and latest date. Use INFORMATION_SCHEMA to dynamically pick up column metadata â€” do not hardcode column names. Round all decimals to 2 places."*

âœ… Full column profile of 1.5M rows. No manual SQL. No hardcoded column names.

---

### ğŸŸ¦ Block 6 â€” Business Analysis
**Describe the business question. Cortex figures out the joins.**

No column names. No table aliases. Just business intent.

| Prompt Intent | Tables Joined | Result |
|---|---|---|
| Net revenue by market segment and region | 5 tables | 25 rows in 2.7s |
| Suppliers with most delayed shipments | 3 tables | Top 10 in 1.5s |
| Part supply margin vs retail price | 2 tables | Top 10 by margin % |

> ğŸ’¡ **Key Insight:** Cortex read the schema metadata, identified correct tables and columns, applied the right formulas â€” all from plain english. No column names were supplied.

---

### ğŸŸ¦ Block 7 â€” dbt Pipeline
**Three prompts. Three models. One working pipeline.**

| Prompt | Output |
|---|---|
| Create target database | DBT_TPCH_ANALYTICS database |
| Configure profiles.yml | Snowflake oauth connection |
| Configure dbt_project.yml | 3 layer materialization strategy |
| Generate sources.yml | Source definitions with descriptions |
| Create stg_orders | Staging view â€” clean and cast |
| Create int_orders_enriched | Intermediate view â€” enriched with geography |
| Create mart_revenue_by_segment | Mart table â€” analytics ready |
| Run all models | PASS=3 WARN=0 ERROR=0 |

> ğŸ’¡ **Key Insight:** Cortex understood dbt conventions â€” ref(), source(), materialization strategies â€” without being taught them. Production-ready pipeline from plain english prompts.

---

## ğŸ’¡ Prompt Writing Best Practices

**1. Set context first â€” always**
Role, warehouse, database, schema before any query.

**2. Use business language not technical language**
Describe what you want, not how to get it. Cortex figures out the columns.

**3. Add cost efficiency instructions**
"Use INFORMATION_SCHEMA", "single query", "do not hardcode column names" â€” nudges Cortex toward smarter, cheaper solutions.

**4. Anticipate Cortex limitations in your prompt**
"Use dbt run not dbt compile" â€” being explicit avoids common mistakes.

**5. One task per prompt**
Scoped prompts produce better output than multi-task prompts.

---

## ğŸ“Š ROI Summary

| Task | Without Cortex Code | With Cortex Code |
|---|---|---|
| Schema understanding | 2-3 hours | 30 seconds |
| Data dictionary | Days | 2 minutes |
| Business queries | Hours | Seconds |
| dbt pipeline setup | Half a day | 15-20 minutes |
| **Total** | **~5 hours** | **~20-30 minutes** |

---

## ğŸ› ï¸ Prerequisites

- Snowflake account with Cortex Code enabled
- Cross-region inference enabled
- ACCOUNTADMIN role or equivalent
- Access to SNOWFLAKE_SAMPLE_DATA database
- dbt project in Snowsight (for Block 7)

---

## ğŸ“– Medium Article

ğŸ“„ **[Read the full walkthrough on Medium](#)**
*(Coming soon)*

---

## ğŸ¤ Connect

**Author:** Rahul Sahay
**LinkedIn:** [[linkedin.com/in/rahulsahay](#)](https://www.linkedin.com/in/rahul-sahay-8573923/)
**GitHub:** [github.com/rahulsahay123](https://github.com/rahulsahay123)

---

â­ **If this helped you â€” give the repo a star and share it with your data engineering network.**
